
Pass 1 (07-08-2019) = itersPerBatch
Character generator: 24 x 24, handwriting only & medium parameters
Hidden layers: 32, 16
Batches: -
Batch size: 4
Iters per batch: -
Learning rate: 0.20

   Iters per batch      Total samples     Avg train loss       Accuracy [%]
                 1              65536             0.7898              61.42
                 2              65536             0.8031              55.39
                 4              65536             0.7784              59.88
                 8              65536             0.7470              54.54
                16              65536             0.7209              49.78
                32              65536             0.6378              47.04
                64              65536             0.5722              36.53
               128              65536             0.4150              35.98
               256              65536             0.3731              24.03

Conclusion: 1 or 4 iterations seems good; definitely not higher as there is little point in feeding the same data over and over again.



Pass 2 (07-08-2019) = learningRate
Character generator: 24 x 24, handwriting only & medium parameters
Hidden layers: 32, 16
Batches: 16384
Batch size: 4
Iters per batch: 1
Learning rate: -

   Learning rate      Total samples     Avg train loss       Accuracy [%]
              0.02              65536             0.9058              11.99
              0.04              65536             0.9027              18.07
              0.06              65536             0.8999              18.20
              0.08              65536             0.8972              20.00
              0.10              65536             0.8869              30.22
              0.12              65536             0.8748              30.46
              0.14              65536             0.8470              41.96
              0.16              65536             0.8395              50.12
              0.18              65536             0.8174              57.86
              0.20              65536             0.8035              54.40
              0.22              65536             0.7757              60.38
              0.24              65536             0.7537              68.64
              0.26              65536             0.7116              73.39
              0.28              65536             0.7025              71.08
              0.30              65536             0.6939              76.54
              0.32              65536             0.6719              76.13
              0.34              65536             0.6445              79.21
              0.36              65536             0.6604              79.36
              0.38              65536             0.6386              79.86
              0.40              65536             0.6205              80.10
              0.42              65536             0.5805              83.98
              0.44              65536             0.6016              83.78
              0.46              65536             0.5507              86.48
              0.48              65536             0.5414              88.09
              0.50              65536             0.5453              86.90
              0.52              65536             0.5712              84.65
              0.54              65536             0.5228              89.03
              0.56              65536             0.5317              86.80
              0.58              65536             0.5267              86.01
              0.60              65536             0.5214              86.12
              0.62              65536             0.4921              87.71
              0.64              65536             0.4955              89.34
              0.66              65536             0.4835              88.36
              0.68              65536             0.4683              86.86
              0.70              65536             0.4744              87.93
              0.72              65536             0.4496              89.87
              0.74              65536             0.4594              90.34
              0.76              65536             0.4620              88.72
              0.78              65536             0.4570              88.72
              0.80              65536             0.4340              90.12
			  
Conclusion: higher is better, up to around 0.60?



Pass 3 (07-08-2019) = batchSize
Character generator: 24 x 24, handwriting only & medium parameters
Hidden layers: 32, 16
Batches: -
Batch size: -
Iters per batch: 1
Learning rate: 0.40

        Batch size      Total samples     Avg train loss       Accuracy [%]
              1.00              65536             0.3461              88.68
              2.00              65536             0.4268              89.53
              3.00              65535             0.5307              86.85
              4.00              65536             0.6508              79.41
              5.00              65535             0.6796              74.11
              6.00              65532             0.7217              73.14
              7.00              65534             0.7720              62.02
              8.00              65536             0.7724              58.70
              9.00              65529             0.8183              54.90
             10.00              65530             0.8440              48.76
             11.00              65527             0.8767              37.93
             12.00              65532             0.8469              44.39
             13.00              65533             0.8570              43.85
             14.00              65534             0.8792              35.71
             15.00              65535             0.8733              31.39
             16.00              65536             0.8888              25.72
             17.00              65535             0.8923              24.35
             18.00              65520             0.8823              26.44
             19.00              65531             0.8787              35.12
             20.00              65520             0.8902              27.64

Conclusion: lower is better, but select >1 for long-term quality.



Pass 4 (07-08-2019) = HL1size
Character generator: 24 x 24, handwriting only & medium parameters
Hidden layers: - (count = 1)
Batches: 16384
Batch size: 4
Iters per batch: 1
Learning rate: 0.40

          HL1 size      Total samples     Avg train loss       Accuracy [%]
                10              65536             0.5944              71.83
                12              65536             0.5535              74.57
                14              65536             0.5482              74.51
                16              65536             0.5218              77.09
                18              65536             0.5185              76.99
                20              65536             0.5085              77.49
                22              65536             0.4978              79.77
                24              65536             0.4851              81.49
                26              65536             0.4755              81.40
                28              65536             0.4800              81.68
                30              65536             0.4712              81.62
                32              65536             0.4706              82.53
                34              65536             0.4558              84.43
                36              65536             0.4615              81.39
                38              65536             0.4579              84.08
                40              65536             0.4716              84.22
                42              65536             0.4552              83.07
                44              65536             0.4418              84.95
                46              65536             0.4414              85.56
                48              65536             0.4491              84.55
                50              65536             0.4371              86.66
                52              65536             0.4439              84.60
                54              65536             0.4392              84.95
                56              65536             0.4361              84.65
                58              65536             0.4381              85.67
                60              65536             0.4354              84.40

Conclusion: higher is better, up to around 40.



Pass 5 (07-08-2019) = HL1size
Character generator: 24 x 24, handwriting only & medium parameters
Hidden layers: -, 16 (count = 2)
Batches: 16384
Batch size: 4
Iters per batch: 1
Learning rate: 0.40

          HL1 size      Total samples     Avg train loss       Accuracy [%]
                10              65536             0.7167              66.61
                12              65536             0.7113              69.89
                14              65536             0.7139              71.16
                16              65536             0.6803              74.92
                18              65536             0.6801              76.72
                20              65536             0.6491              77.68
                22              65536             0.6462              78.26
                24              65536             0.6429              78.71
                26              65536             0.6105              80.70
                28              65536             0.6291              79.70
                30              65536             0.6142              82.75
                32              65536             0.5984              83.27
                34              65536             0.5909              83.22
                36              65536             0.6117              83.79
                38              65536             0.6100              83.50
                40              65536             0.5818              83.73
                42              65536             0.5898              82.94
                44              65536             0.5780              85.39
                46              65536             0.6064              82.23
                48              65536             0.5819              82.76
                50              65536             0.5961              81.58
                52              65536             0.5688              84.23
                54              65536             0.5723              84.34
                56              65536             0.5857              83.38
                58              65536             0.5529              85.28
                60              65536             0.5691              85.42

Conclusion: higher is better, up to around 32.



Pass 6 (07-08-2019) = HL2size
Character generator: 24 x 24, handwriting only & medium parameters
Hidden layers: 32, - (count = 2)
Batches: 16384
Batch size: 4
Iters per batch: 1
Learning rate: 0.40

          HL2 size      Total samples     Avg train loss       Accuracy [%]
                 4              65536             0.8147              48.19
                 6              65536             0.7757              62.21
                 8              65536             0.7431              66.24
                10              65536             0.6582              79.75
                12              65536             0.6603              78.78
                14              65536             0.6187              84.52
                16              65536             0.6060              80.49
                18              65536             0.6238              79.58
                20              65536             0.6015              82.66
                22              65536             0.5750              83.40
                24              65536             0.5970              81.10
                26              65536             0.5723              79.22
                28              65536             0.5656              82.83
                30              65536             0.5820              82.46
                32              65536             0.5567              84.24
                34              65536             0.5603              81.09
                36              65536             0.5521              85.17
                38              65536             0.5425              83.40
                40              65536             0.5598              84.39
				
Conclusion: higher is better, up to around 16.
